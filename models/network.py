import torch
import torch.nn as nn
from models.util_wqaq import Conv2d_Q,Linear_Q,MaxPool_Q,Relu_Q,BNFold_Conv2d_Q
class Net(nn.Module):
    def __init__(self, abits=8, wbits=8, q_type=1):
        super(Net, self).__init__()
        self.conv1_1 = BNFold_Conv2d_Q(3, 64, kernel_size=3, stride=1, padding=1,a_bits=abits, w_bits=wbits, q_type=q_type, first_layer=1)
        self.relu1_1 = Relu_Q(64,64,a_bits=abits, q_type=q_type)
        self.conv1_2 = BNFold_Conv2d_Q(64, 64, kernel_size=3, stride=1, padding=1,a_bits=abits, q_type=q_type, w_bits=wbits)
        self.relu1_2 = Relu_Q(64, 64,a_bits=abits, q_type=q_type)
        self.maxpool_1 = MaxPool_Q(64,64,a_bits=abits, q_type=q_type)
        self.conv2_1= BNFold_Conv2d_Q(64, 128, kernel_size=3, stride=1, padding=1,a_bits=abits, q_type=q_type, w_bits=wbits)
        self.relu2_1 = Relu_Q(128,128,a_bits=abits, q_type=q_type)
        self.conv2_2= BNFold_Conv2d_Q(128, 128, kernel_size=3, stride=1, padding=1,a_bits=abits, w_bits=wbits, q_type=q_type)
        self.relu2_2 = Relu_Q(128,128,a_bits=abits, q_type=q_type)
        self.maxpool_2 = MaxPool_Q(128,128,a_bits=abits, q_type=q_type)
        self.conv3_1= BNFold_Conv2d_Q(128, 256, kernel_size=3, stride=1, padding=1,a_bits=abits, w_bits=wbits, q_type=q_type)
        self.relu3_1 = Relu_Q(256,256,a_bits=abits, q_type=q_type)
        self.conv3_2= BNFold_Conv2d_Q(256, 256, kernel_size=3, stride=1, padding=1,a_bits=abits, w_bits=wbits, q_type=q_type)
        self.relu3_2 = Relu_Q(256,256,a_bits=abits, q_type=q_type)
        self.conv3_3= BNFold_Conv2d_Q(256, 256, kernel_size=3, stride=1, padding=1,a_bits=abits, w_bits=wbits, q_type=q_type)
        self.relu3_3 = Relu_Q(256,256,a_bits=abits, q_type=q_type)
        self.maxpool_3 = MaxPool_Q(256,256,a_bits=abits, q_type=q_type)
        self.fc1 = Linear_Q(256,4096,a_bits=abits, w_bits=wbits, q_type=q_type)
        self.relu5_1 = Relu_Q(4096,4096,a_bits=abits, q_type=q_type)
        self.fc2 = Linear_Q(4096,4096,a_bits=abits, w_bits=wbits, q_type=q_type)
        self.relu5_2 = Relu_Q(4096,4096,a_bits=abits, q_type=q_type)
        self.fc3 = Linear_Q(4096,2,a_bits=abits, w_bits=wbits, q_type=q_type)
    
    def forward(self,x):
        x = self.conv1_1(x)
        x = self.relu1_1(x)
        x = self.conv1_2(x)
        x = self.relu1_2(x)
        x = self.maxpool_1(x)
        x = self.conv2_1(x)
        x = self.relu2_1(x)
        x = self.conv2_2(x)
        x = self.relu2_2(x)
        x = self.maxpool_2(x)
        x = self.conv3_1(x)
        x = self.relu3_1(x)
        x = self.conv3_2(x)
        x = self.relu3_2(x)
        x = self.conv3_3(x)
        x = self.relu3_3(x)
        x = self.maxpool_3(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu5_1(x)
        x = self.fc2(x)
        x = self.relu5_2(x)
        x = self.fc3(x)
        return x